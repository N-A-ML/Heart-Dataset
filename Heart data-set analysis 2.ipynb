{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to predict whether or not a person has heart disease, using the dataset from https://www.kaggle.com/ronitf/heart-disease-uci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing some required packages, reading the data in and splitting data into X (features) and y (target) dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'Desktop\\KaggleDatasets\\heart.csv') #read data into python\n",
    "\n",
    "df.columns=[\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\"fbs\",\"restecg\",\"thalach\",\"exang\",\"oldpeak\",\"slope\",\"ca\",\"thal\",\"heart_disease\"] #explicitly define column names and relabel target as heart_disease\n",
    "df[(np.abs(stats.zscore(df)) < 3).all(axis=1)] #removes outliers, code from https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
    "df.dropna()\n",
    "df = pd.get_dummies(df,columns=['sex','cp','fbs','restecg','exang','slope','ca','thal'])\n",
    "\n",
    "X=df.loc[:,df.columns!=\"heart_disease\"] #let the feature dataframe contain every column of df, except the value we are predicting, heart_disease\n",
    "y=df.loc[:,df.columns==\"heart_disease\"].values.ravel() #let the target dataframe contain only the value we are predicting, heart_disease\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the best features to use for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best predictors are ['cp_0', 'exang_1', 'ca_0', 'thal_2', 'thal_3']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "\n",
    "k=5 #k=5 leads to the best models\n",
    "SKB = SelectKBest(f_classif,k=k) #select the 5 best features of X\n",
    "SKB.fit_transform(X, y) #fit the data to SKB\n",
    "\n",
    "X_best_indices=SKB.get_support(indices=True) \n",
    "\n",
    "best_predictors=[None]*(k) #k best predictors\n",
    "for i in range(0,k):\n",
    "    best_predictors[i]=X.columns[X_best_indices[i]] #create list of names of best predictors\n",
    "       \n",
    "print('The best predictors are',best_predictors)  #print names of best predictors \n",
    "\n",
    "X=X[best_predictors].copy() #use list to create new X dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the data into training and test sets and transform features using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.20,random_state=42)                                            \n",
    "                                                                                              \n",
    "                                    \n",
    "ss=StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train=ss.transform(X_train)\n",
    "X_test=ss.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there may have been a nonlinear relationship between X and y, we tested various degrees of PolynomialFeatures.  However, logistic regression has the highest accuracy when the features have a degree of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score with X to the power of 1 is 0.9016393442622951\n",
      "The accuracy score with X to the power of 2 is 0.8360655737704918\n",
      "The accuracy score with X to the power of 3 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 4 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 5 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 6 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 7 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 8 is 0.8524590163934426\n",
      "The accuracy score with X to the power of 9 is 0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score  #polynomial features aren't useful in these case\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "for i in range(1,10):\n",
    "    poly=PolynomialFeatures(i)\n",
    "    poly.fit(X_train)\n",
    "    X_train_poly=poly.transform(X_train)\n",
    "    X_test_poly=poly.transform(X_test)\n",
    "    Logreg=LogisticRegression(solver='lbfgs',max_iter=4000)\n",
    "    Logreg.fit(X_train_poly,y_train)\n",
    "    accscore=accuracy_score(y_test,Logreg.predict(X_test_poly))\n",
    "    print('The accuracy score with X to the power of {} is {}'.format(i,accscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a logistic regression model and fit it to the data.  Since the classes are not very imbalanced, we check the accuracy of the model on the test set. We also use GridSearch to find the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracies for logistic regression on the training and tests sets are, respectively, 0.8388429752066116 and 0.9016393442622951\n",
      "The accuracies for logistic regression from grid search on the training and tests sets are, respectively, 0.8388429752066116 and 0.9016393442622951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        29\n",
      "           1       0.91      0.91      0.91        32\n",
      "\n",
      "   micro avg       0.90      0.90      0.90        61\n",
      "   macro avg       0.90      0.90      0.90        61\n",
      "weighted avg       0.90      0.90      0.90        61\n",
      "\n",
      "[[26  3]\n",
      " [ 3 29]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "logreg=LogisticRegression(solver='lbfgs')\n",
    "                         #define linear regression model\n",
    "logreg.fit(X_train,y_train) #fit data to the linear regression model\n",
    "\n",
    "y_pred_train=logreg.predict(X_train)\n",
    "train_accuracy=accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test=logreg.predict(X_test)\n",
    "test_accuracy=accuracy_score(y_test,y_pred_test)\n",
    "print('The accuracies for logistic regression on the training and tests sets are, respectively, {} and {}'.format(train_accuracy,test_accuracy))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "grid={'penalty' : [#'l1', \n",
    "    'l2'], #https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\n",
    "    'C' : [0.0001,0.001,0.01,0.1,1,10,100,1000],\n",
    "    'solver' : [#'liblinear' \n",
    "                     'lbfgs'\n",
    "               ]}\n",
    "\n",
    "logreg_GS=GridSearchCV(estimator=logreg,param_grid=grid,cv=5,n_jobs=-1,refit=True).fit(X_train,y_train)\n",
    "\n",
    "y_pred_train_GS=logreg_GS.predict(X_train)\n",
    "train_accuracy_GS=accuracy_score(y_train, y_pred_train_GS)\n",
    "\n",
    "y_pred_test_GS=logreg_GS.predict(X_test)\n",
    "test_accuracy_GS=accuracy_score(y_test,y_pred_test_GS)\n",
    "print('The accuracies for logistic regression from grid search on the training and tests sets are, respectively, {} and {}'.format(train_accuracy_GS,test_accuracy_GS))\n",
    "\n",
    "print(classification_report(y_test,y_pred_test_GS))\n",
    "print(confusion_matrix(y_test,y_pred_test_GS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a support vector classifier in a similar way that we used logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524590163934426\n",
      "0.9016393442622951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        29\n",
      "           1       0.91      0.91      0.91        32\n",
      "\n",
      "   micro avg       0.90      0.90      0.90        61\n",
      "   macro avg       0.90      0.90      0.90        61\n",
      "weighted avg       0.90      0.90      0.90        61\n",
      "\n",
      "[[26  3]\n",
      " [ 3 29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score, f1_score,classification_report,confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc=svm.SVC(gamma='auto')\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "y_pred_test=svc.predict(X_test)\n",
    "#confusionmatrix=sklearn.metrics.confusion_matrix(y_test, y_pred_test)\n",
    "accscore=accuracy_score(y_test, y_pred_test)\n",
    "print(accscore)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parameters=[{'gamma':[1,0,0.1,0.001,0.0001],'C':[1,10,100,1000],'kernel':['linear','rbf']}]\n",
    "svm=svm.SVC(gamma='auto')\n",
    "svc_GS = GridSearchCV(svm, parameters,cv=5,iid='parameter',refit=True)\n",
    "\n",
    "svc_GS.fit(X_train,y_train)\n",
    "\n",
    "y_pred_test_GS=svc_GS.predict(X_test)\n",
    "accscore_GS=accuracy_score(y_test, y_pred_test_GS)\n",
    "print(accscore_GS)\n",
    "\n",
    "print(classification_report(y_test,y_pred_test_GS))\n",
    "print(confusion_matrix(y_test,y_pred_test_GS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined and trained a random forest with default hyperparameters to predict target. We then performed an exhaustive randomised search using a grid with wide ranges of hyperparameter values (omitted). Then we repeat the search with a slightly narrowed down grid of values.  Note: this code will have a different output each time, due to the stochastic nature of boostrap sampling and randomised search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1428 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3928 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:   41.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=False, n_iter=1000, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n",
       "       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       61, 62, 63, 64, 65, 66, 67, 68, 69]), 'max_features': ['auto', 'sqrt'], 'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'min_samples_split': [2, 3, 4, 5], 'min_samples_leaf': [1, 2, 3, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "rf=RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rf.fit(X_train,y_train) \n",
    "\n",
    "\n",
    "#below, we use wide ranges of parameter values for our random search\n",
    "\n",
    "random_grid = {'n_estimators': np.arange(10,70,1),\n",
    "               'max_features': ['auto','sqrt'],\n",
    "               'max_depth': np.arange(1,10,1),\n",
    "               'min_samples_split': [2,3,4,5],\n",
    "               'min_samples_leaf': [1,2,3,4],\n",
    "               'bootstrap': [True,False]}\n",
    "\n",
    "rf_RS = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 1000, cv =5, verbose=True, iid=False,scoring='accuracy',n_jobs=-1) #does a randomised search and finds 'best' parameters\n",
    "rf_RS.fit(X_train, y_train) #fit for each set of parameters\n",
    "\n",
    "\n",
    "print(rf_RS.best_params_,rf_RS.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random forest classifier with the 'best' hyperparameters found from one execution of the code above:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for rf on test data is 0.8524590163934426\n",
      "Accuracy scores for rf_RS on train data and test data are,respectively 0.8553719008264463 and 0.8524590163934426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_RS_best_params={'bootstrap': True, 'n_estimators': 15, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'max_depth': 5}\n",
    "\n",
    "if 'bootstrap' in rf_RS_best_params:\n",
    "       del rf_RS_best_params['bootstrap']\n",
    "rf_RS=RandomForestClassifier(**rf_RS_best_params)\n",
    "rf_RS.fit(X_train,y_train)\n",
    "print('Accuracy score for rf on test data is',rf_RS.score(X_test,y_test))\n",
    "print('Accuracy scores for rf_RS on train data and test data are,respectively {} and {}'.format(rf_RS.score(X_train,y_train),rf_RS.score(X_test,y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_GS score on training data is 0.8471074380165289\n",
      "rf_GS score on test data is  0.9016393442622951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "n_estimators =np.arange(10,45,1) #https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = np.arange(1,25,1)\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,3,4,5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,2,3,4]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True,False]\n",
    "rf2=RandomForestClassifier(**rf_RS_best_params)\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_GS=GridSearchCV(estimator=rf2,param_grid=grid,cv=5,n_jobs=-1,refit=True).fit(X_train,y_train)\n",
    "\n",
    "print(rf_RS.best_params_,rf_GS.best_score_)\n",
    "print('rf_GS score on training data is',accuracy_score(y_train,rf_GS.predict(X_train))) \n",
    "print('rf_GS score on test data is ',accuracy_score(y_test,rf_GS.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a random forest classifier with the 'best' hyperparameters found from one execution of the code above:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_GS score on training data is 0.8512396694214877\n",
      "rf_GS score on test data is  0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "rf_GS_best_params={'bootstrap': True, 'max_depth': 2, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 25}\n",
    "\n",
    "if 'bootstrap' in rf_GS_best_params:\n",
    "    del rf_GS_best_params['bootstrap']\n",
    "        \n",
    "rf_GS=RandomForestClassifier(**rf_GS_best_params)\n",
    "rf_GS.fit(X_train,y_train)\n",
    "print('rf_GS score on training data is',accuracy_score(y_train,rf_GS.predict(X_train))) \n",
    "print('rf_GS score on test data is ',accuracy_score(y_test,rf_GS.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use a gradient boosting classifier with the hyperparameters found from the gridsearch.  We found that 0.1 was the optimal learning rate, and used another gridsearch to find the optimal max depth and number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc score on training data is 0.8429752066115702\n",
      "gbc score on test data is  0.8360655737704918\n",
      "gbc baseline score on training data and test data are, respectively 0.8553719008264463 and 0.8524590163934426 \n",
      "{'max_depth': 1, 'n_estimators': 24}\n",
      "gbc_GS score on training data is 0.8264462809917356\n",
      "gbc_GS score on test data is  0.9016393442622951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natha\\AppData\\Local\\Continuum\\anaconda3\\I2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV    \n",
    "\n",
    "\n",
    "rf_GS_best_params={'bootstrap': True, 'max_depth': 2, 'max_features': 'auto', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 25}\n",
    "\n",
    "\n",
    "if 'bootstrap' in rf_GS_best_params:\n",
    "    del rf_GS_best_params['bootstrap']\n",
    "        \n",
    "gbc=GradientBoostingClassifier(**rf_GS_best_params, learning_rate=0.1).fit(X_train,y_train)                         \n",
    "                              \n",
    "\n",
    "print('gbc score on training data is',accuracy_score(y_train,gbc.predict(X_train))) \n",
    "print('gbc score on test data is ',accuracy_score(y_test,gbc.predict(X_test)))\n",
    "\n",
    "\n",
    "gbc_baseline=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "\n",
    "print('gbc baseline score on training data and test data are, respectively {} and {} '.format(gbc_baseline.score(X_train,y_train),gbc_baseline.score(X_test,y_test))) \n",
    "\n",
    "\n",
    "grid={'n_estimators':np.arange(1,42,1),\n",
    "    'max_depth':[1,2,3,9]}\n",
    "\n",
    "\n",
    "gbc_GS=GridSearchCV(estimator=gbc,param_grid=grid,cv=5,refit=True,n_jobs=-1,scoring='accuracy').fit(X_train,y_train)\n",
    "\n",
    "print(gbc_GS.best_params_)\n",
    "print('gbc_GS score on training data is',accuracy_score(y_train,gbc_GS.predict(X_train))) \n",
    "print('gbc_GS score on test data is ',accuracy_score(y_test,gbc_GS.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a voting classifier to combine some of the models we have used so far and find that we have good accuray (90.16% on test data), although this is not an improvement on the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8512396694214877\n",
      "0.9016393442622951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  VotingClassifier\n",
    "\n",
    "svc_GS.probability=True\n",
    "vc= VotingClassifier(estimators=[\n",
    "('logreg_GS', logreg_GS),\n",
    "('svc_GS',svc_GS),\n",
    "('rf_GS',rf_GS ),    \n",
    "('gbc_GS',gbc)\n",
    "                                    ]\n",
    ",voting='hard',n_jobs=-1)\n",
    "vc.fit(X_train,y_train)\n",
    "votingclassifier(X_train,y_train)\n",
    "\n",
    "y_pred_train=vc.predict(X_train)\n",
    "y_pred_test=vc.predict(X_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_train,y_pred_train))  #0.8512396694214877\n",
    "\n",
    "print(accuracy_score(y_test,y_pred_test))    #0.9016393442622951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        29\n",
      "           1       0.91      0.91      0.91        32\n",
      "\n",
      "   micro avg       0.90      0.90      0.90        61\n",
      "   macro avg       0.90      0.90      0.90        61\n",
      "weighted avg       0.90      0.90      0.90        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test,y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
